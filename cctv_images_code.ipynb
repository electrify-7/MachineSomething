{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow opencv-python Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import os\n",
    "\n",
    "# Load the low-quality CCTV image (Replace with your own dataset path)\n",
    "img_path = 'path_to_your_low_quality_cctv_image.jpg'\n",
    "low_quality_image = cv2.imread(img_path)\n",
    "\n",
    "# Resize for input to the model if necessary\n",
    "low_quality_image = cv2.resize(low_quality_image, (128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super-Resolution Using SRCNN (Super-Resolution Convolutional Neural Network)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# Define a simple SRCNN model for super-resolution\n",
    "def build_srcnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Layer\n",
    "    model.add(Conv2D(64, (9, 9), activation='relu', padding='same', input_shape=(None, None, 3)))\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu', padding='same'))\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    model.add(Conv2D(3, (5, 5), padding='same'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the model\n",
    "srcnn_model = build_srcnn_model()\n",
    "\n",
    "# Train the model on the low-res/high-res dataset (Here we would use the provided dataset)\n",
    "# srcnn_model.fit(x_train_low_res, y_train_high_res, epochs=10, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Image Denoising and Deblurring\n",
    "# Function to denoise the image using a Gaussian filter\n",
    "def denoise_image(image):\n",
    "    return cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Function to deblur the image using Wiener filter\n",
    "def deblur_image(image):\n",
    "    deblurred = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
    "    return deblurred\n",
    "\n",
    "# Apply denoising and deblurring\n",
    "denoised_image = denoise_image(low_quality_image)\n",
    "deblurred_image = deblur_image(denoised_image)\n",
    "\n",
    "# Save the processed images\n",
    "cv2.imwrite('denoised_image.jpg', denoised_image)\n",
    "cv2.imwrite('deblurred_image.jpg', deblurred_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 5: Model for Face Reconstruction (Using Autoencoders or GANs)\n",
    "from tensorflow.keras.layers import Input, UpSampling2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define a simple autoencoder for image reconstruction\n",
    "def build_autoencoder():\n",
    "    input_img = Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return autoencoder\n",
    "\n",
    "# Build and compile the autoencoder\n",
    "autoencoder = build_autoencoder()\n",
    "\n",
    "# Train the autoencoder on low-res / high-res dataset pairs\n",
    "# autoencoder.fit(x_train_low_res, y_train_high_res, epochs=20, batch_size=16)\n",
    "\n",
    "# Test the autoencoder with a low-quality image\n",
    "reconstructed_image = autoencoder.predict(np.expand_dims(low_quality_image, axis=0))\n",
    "\n",
    "# Save the reconstructed image\n",
    "cv2.imwrite('reconstructed_image.jpg', reconstructed_image[0] * 255)  # Convert to image scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Comparing the Original and Reconstructed Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load original and reconstructed images for comparison\n",
    "original_image = cv2.imread('path_to_original_cctv_image.jpg')\n",
    "reconstructed_image = cv2.imread('reconstructed_image.jpg')\n",
    "\n",
    "# Show original vs reconstructed\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(reconstructed_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Reconstructed Image')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Real-Time Processing Capability (Optional)\n",
    "# Capture video from CCTV feed\n",
    "cap = cv2.VideoCapture('path_to_video_feed')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process each frame (denoise, deblur, reconstruct)\n",
    "    frame_denoised = denoise_image(frame)\n",
    "    frame_deblurred = deblur_image(frame_denoised)\n",
    "    reconstructed_frame = autoencoder.predict(np.expand_dims(frame_deblurred, axis=0))[0]\n",
    "\n",
    "    # Display the processed frame\n",
    "    cv2.imshow('Reconstructed Frame', reconstructed_frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
